{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Modelling"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "source": [
    "# Importing Libraries\r\n",
    "from sklearn.model_selection import KFold\r\n",
    "from sklearn.model_selection import cross_val_score\r\n",
    "from sklearn.linear_model import LogisticRegression\r\n",
    "from causalnex.structure import DAGClassifier\r\n",
    "import pandas as pd\r\n",
    "import numpy as np\r\n",
    "import sys\r\n",
    "from typing import Tuple\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "# DataFrame Splitter\r\n",
    "def split_dataframe(df: pd.DataFrame, percentage: float = 0.2) -> Tuple[pd.DataFrame, pd.DataFrame]:\r\n",
    "    try:\r\n",
    "        df_size = len(df)\r\n",
    "        cut_size = df_size - int(df_size * (1 - percentage))\r\n",
    "        train_df = df.iloc[:cut_size, :]\r\n",
    "        test_df = df.iloc[cut_size:, :]\r\n",
    "\r\n",
    "        return (train_df, test_df)\r\n",
    "\r\n",
    "    except Exception as e:\r\n",
    "        print('Failed to Split Dataframe to Train and Test Segments')\r\n",
    "        sys.exit(1)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "def separate_x_y(df: pd.DataFrame, target_index:int, start_index: int = 0) -> Tuple[pd.DataFrame, pd.DataFrame]:\r\n",
    "    x = df.iloc[:, start_index:]\r\n",
    "    y = df.iloc[:, target_index]\r\n",
    "\r\n",
    "    return (x, y)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Loading and Preparing The Dataset for training"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Dataset One"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "# MinMax Outliear Removed Norm\r\n",
    "data_1 = pd.read_csv('../data/out_removed_minmax_scale.csv')\r\n",
    "data_1_train, data_1_test = split_dataframe(data_1, percentage=0.2)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "print(\"Total Data Size:\\n\\t\\t\",len(data_1))\r\n",
    "print(\"Train Data Size:\\n\\t\\t\", len(data_1_train))\r\n",
    "print(\"Test Data Size:\\n\\t\\t\", len(data_1_test))\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Total Data Size:\n",
      "\t\t 483\n",
      "Train Data Size:\n",
      "\t\t 97\n",
      "Test Data Size:\n",
      "\t\t 386\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "data_1_train_x, data_1_train_y = separate_x_y(data_1_train, 1, 2)\r\n",
    "data_1_test_x, data_1_test_y = separate_x_y(data_1_test, 1, 2)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Dataset Two"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "# MinMax Outliear Revaluated Norm\r\n",
    "data_2 = pd.read_csv('../data/out_revalued_minmax_scale.csv')\r\n",
    "data_2_train, data_2_test = split_dataframe(data_2, percentage=0.2)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "print(\"Total Data Size:\\n\\t\\t\", len(data_2))\r\n",
    "print(\"Train Data Size:\\n\\t\\t\", len(data_2_train))\r\n",
    "print(\"Test Data Size:\\n\\t\\t\", len(data_2_test))\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Total Data Size:\n",
      "\t\t 569\n",
      "Train Data Size:\n",
      "\t\t 114\n",
      "Test Data Size:\n",
      "\t\t 455\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "data_2_train_x, data_2_train_y = separate_x_y(data_2_train, 1, 2)\r\n",
    "data_2_test_x, data_2_test_y = separate_x_y(data_2_test, 1, 2)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Modelling Using all Feature"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Training on All Features Using Dataset One Using Simple Logistic Regression"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "source": [
    "names = data_1_train_x.columns.to_list()\r\n",
    "\r\n",
    "lr_model_d1 = LogisticRegression()\r\n",
    "scores = cross_val_score(lr_model_d1, data_1_train_x, data_1_train_y, cv=KFold(shuffle=True, random_state=42))\r\n",
    "print(f'MEAN Score: {np.mean(scores).mean():.3f}')\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "MEAN Score: 0.959\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "source": [
    "lr_model_d1.fit(data_1_train_x, data_1_train_y)\r\n"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "metadata": {},
     "execution_count": 30
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "source": [
    "for i in range(lr_model_d1.coef_.shape[0]):\r\n",
    "    print(\"MEAN EFFECT DIRECTIONAL CLASS {}:\".format(i))\r\n",
    "    print(pd.Series(lr_model_d1.coef_[i, :], index=names).sort_values(ascending=False))\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "MEAN EFFECT DIRECTIONAL CLASS 0:\n",
      "concave points_worst          1.206861\n",
      "texture_mean                  1.018258\n",
      "texture_worst                 1.001229\n",
      "perimeter_worst               0.999552\n",
      "radius_worst                  0.995210\n",
      "area_worst                    0.938930\n",
      "concavity_worst               0.906180\n",
      "concave points_mean           0.874125\n",
      "perimeter_mean                0.829652\n",
      "concavity_mean                0.814758\n",
      "radius_mean                   0.791623\n",
      "area_mean                     0.754372\n",
      "area_se                       0.751480\n",
      "compactness_worst             0.735918\n",
      "radius_se                     0.715506\n",
      "perimeter_se                  0.701816\n",
      "concavity_dispersion_mean     0.633102\n",
      "fractal_dimension_worst       0.519871\n",
      "compactness_mean              0.495338\n",
      "concave points_se             0.411350\n",
      "symmetry_worst                0.398810\n",
      "smoothness_worst              0.380628\n",
      "concavity_se                  0.254372\n",
      "concavity_dispersion_worst    0.105912\n",
      "symmetry_mean                 0.062044\n",
      "smoothness_mean               0.035315\n",
      "compactness_se               -0.044798\n",
      "texture_se                   -0.115082\n",
      "fractal_dimension_se         -0.286534\n",
      "fractal_dimension_mean       -0.303038\n",
      "smoothness_se                -0.320126\n",
      "symmetry_se                  -0.529356\n",
      "concavity_dispersion_se      -0.879473\n",
      "dtype: float64\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Training on All Features Using Dataset One Using DAG Classifier"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "source": [
    "dag_model_d1 = DAGClassifier(\r\n",
    "    alpha=0.1,\r\n",
    "    beta=0.9,\r\n",
    "    hidden_layer_units=[5],\r\n",
    "    fit_intercept=True,\r\n",
    "    standardize=True\r\n",
    ")\r\n",
    "scores = cross_val_score(dag_model_d1, data_1_train_x,\r\n",
    "                         data_1_train_y, cv=KFold(shuffle=True, random_state=42))\r\n",
    "print(f'MEAN Score: {np.mean(scores).mean():.3f}')\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "MEAN Score: 0.918\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "source": [
    "dag_model_d1.fit(data_1_train_x, data_1_train_y)\r\n"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DAGClassifier(alpha=0.1, beta=0.9, hidden_layer_units=[5], standardize=True,\n",
       "              target_dist_type='bin')"
      ]
     },
     "metadata": {},
     "execution_count": 37
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "source": [
    "for i in range(lr_model_d1.coef_.shape[0]):\r\n",
    "    print(\"MEAN EFFECT DIRECTIONAL CLASS {}:\".format(i))\r\n",
    "    print(pd.Series(lr_model_d1.coef_[i, :],\r\n",
    "                    index=names).sort_values(ascending=False))\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "MEAN EFFECT DIRECTIONAL CLASS 0:\n",
      "concave points_worst          1.206861\n",
      "texture_mean                  1.018258\n",
      "texture_worst                 1.001229\n",
      "perimeter_worst               0.999552\n",
      "radius_worst                  0.995210\n",
      "area_worst                    0.938930\n",
      "concavity_worst               0.906180\n",
      "concave points_mean           0.874125\n",
      "perimeter_mean                0.829652\n",
      "concavity_mean                0.814758\n",
      "radius_mean                   0.791623\n",
      "area_mean                     0.754372\n",
      "area_se                       0.751480\n",
      "compactness_worst             0.735918\n",
      "radius_se                     0.715506\n",
      "perimeter_se                  0.701816\n",
      "concavity_dispersion_mean     0.633102\n",
      "fractal_dimension_worst       0.519871\n",
      "compactness_mean              0.495338\n",
      "concave points_se             0.411350\n",
      "symmetry_worst                0.398810\n",
      "smoothness_worst              0.380628\n",
      "concavity_se                  0.254372\n",
      "concavity_dispersion_worst    0.105912\n",
      "symmetry_mean                 0.062044\n",
      "smoothness_mean               0.035315\n",
      "compactness_se               -0.044798\n",
      "texture_se                   -0.115082\n",
      "fractal_dimension_se         -0.286534\n",
      "fractal_dimension_mean       -0.303038\n",
      "smoothness_se                -0.320126\n",
      "symmetry_se                  -0.529356\n",
      "concavity_dispersion_se      -0.879473\n",
      "dtype: float64\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Modelling Using Only Directly connected Features"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "source": [
    "# Declaring Training Used Features\r\n",
    "features = ['radius_mean','radius_se'] # Change here"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "source": [
    "# Selecting Features from Dataset One\r\n",
    "data_1_train_x_feat = data_1_train_x[features]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Training on Selected Features Using Dataset One Using Simple Logistic Regression"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Training on Selected Features Using Dataset One Using DAG Classifier"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.4",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.4 64-bit ('causalnex': virtualenv)"
  },
  "interpreter": {
   "hash": "6244ac4e6c13dccc799912dc74a9701d27066b1f2f1f2953cd7e13f45253ab72"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}